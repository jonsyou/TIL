# SMOTE를 통한 데이터 불균형 처리

## 오늘 공부한 것
예전 항공지연예측에 대한 공모전에 참가하여 데이터 불균형에 대해 처음으로 관심있게 찾아본적이 있었다. 그때 당시 ```target``` 변수인 지연 여부에 대한 정보가 불균형한 형태를 가지고 있어서 예측을 위해 언더샘플링, 오버샘플링에 대해 찾아본 기억이 있다. 이를 잘 정리해두면 이후에 이같은 상황을 맞이하였을 때 하나의 대처 방법으로 빠르게 고려해볼 수 있다고 생각한다. 그리하여 오늘은  데이터 불균형을 처리 하기 위한 기법들을 다시 공부해보았다. 

## 개요
데이터 분석을 하다보면 비교적 쉽게 데이터 불균형의 경우를 접할 수 있다. 예를 들면 기업 부도 예측, 혹은 심장병 발생 여부 예측 등의 경우에 ```target``` 변수가 불균형한 경우이다.

이러한 비대칭 데이터의 경우엔 정확도(accuracy)가 아무리 높더라도 재현율(recall)이 급격히 낮아질 수 있다. 

쉽게 예를 들자면 100명중 3명이 심장병 환자일 때 심장병 환자를 예측하는 경우, 모두 정상이라고 예측해버리면 정확도가 97%가 나온다. 그러나 우리는 심장병 환자를 제대로 예측하여 병이 더 악화되기전에 막고자 하는데에 의의가 있다. 즉, 심장병 환자를 정확히 예측하는 비율인 재현율(recall)이 정확도(accuracy)보다 중요하다고 할 수 있다. 하지만 비대칭 데이터이다 보니 재현율 낮아질 수 있다. 이에대한 대처로 현존하는 데이터 불균형 처리 기법들을 알아보고자 한다.

## 데이터 불균형 처리 방법
### 언더 샘플링
1. 무작위추출 : 무작위로 정상 데이터 일부만 선택
2. 유의정보 : 유의한 데이터만 남기는 방식

언더 샘플링의 경우 데이터의 소실이 크고, 때로는 중요 정상 데이터를 잃게 될 가능성이 있다.

![image](https://user-images.githubusercontent.com/74973306/114308883-4ee7b980-9b20-11eb-8250-50733f35115a.png)

### 오버 샘플링
1. 무작위추출 : 무작위로 소수 데이터를 복제
2. 유의정보 : 사전 기준을 정해 소수 데이터 복제  
- 정보가 손실되지 않는다는 장점이 있으나, 복제된 관측치를 원래 데이터 세트에 추가하기 만하면 여러 유형의 관측치를 다수 추가하여 오버 피팅 (overfitting)을 초래할 수 있다. 
- 이러한 경우 trainset의 성능은 높으나 testset의 성능은 나빠질 수 있다.

3. 합성 데이터 생성 : 소수 데이터를 단순 복제하는 것이 아니라 새로운 복제본을 만들어 낸다.

![image](https://user-images.githubusercontent.com/74973306/114309032-c3225d00-9b20-11eb-823b-556096f97040.png)



> 참고 : https://mkjjo.github.io/python/2019/01/04/smote_duplicate.html
